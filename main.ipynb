{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv('dataset.csv', delimiter=',')\n",
    "questions = df['question'].tolist()[:int(os.getenv(\"HOW_MANY\"))]\n",
    "ground_truths = df['correct'].tolist()[:int(os.getenv(\"HOW_MANY\"))]\n",
    "\n",
    "indeces = df['id'].tolist()[:int(os.getenv(\"HOW_MANY\"))]\n",
    "\n",
    "filenames = os.listdir(os.getenv(\"LOCAL_FILE_INPUT_DIR\"))\n",
    "\n",
    "    #filepaths = [os.path.join(os.getenv(\"LOCAL_FILE_INPUT_DIR\"), filename) for filename in filenames if not filename.startswith(\".\")]\n",
    "filepaths = [os.path.join(os.getenv(\"LOCAL_FILE_INPUT_DIR\"), filename) for filename in filenames if os.path.splitext(filename)[0] in indeces]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Warming up component sparse_doc_embedder...\n",
      "INFO: Warming up component dense_doc_embedder...\n",
      "INFO: Running component converter\n",
      "Converting files to Haystack Documents: 0it [00:00, ?it/s]WARNING: 'split_pdf_cache_tmp_data' does not exist. Using default value '/tmp'.\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 402 Payment Required\"\n",
      "ERROR: Failed to partition the document.\n",
      "ERROR: Server responded with 402 - {\"detail\":\"Insufficient API quota for owner organization\"}\n",
      "WARNING: Unstructured could not process file data/56c0706e-ad07-4ad0-9128-86e16b734f80.pdf. Error: API error occurred: Status 402\n",
      "{\"detail\":\"Insufficient API quota for owner organization\"}\n",
      "Converting files to Haystack Documents: 1it [00:01,  1.30s/it]WARNING: 'split_pdf_cache_tmp_data' does not exist. Using default value '/tmp'.\n",
      "INFO: HTTP Request: GET https://api.unstructuredapp.io/general/docs \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 402 Payment Required\"\n",
      "ERROR: Failed to partition set 2.\n",
      "WARNING: Unstructured could not process file data/629bcacf-0c6d-4594-bb47-f48d3796c53a.pdf. Error: {\"detail\":\"Insufficient API quota for owner organization\"}\n",
      "Converting files to Haystack Documents: 2it [00:03,  1.63s/it]WARNING: 'split_pdf_cache_tmp_data' does not exist. Using default value '/tmp'.\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 402 Payment Required\"\n",
      "ERROR: Failed to partition the document.\n",
      "ERROR: Server responded with 402 - {\"detail\":\"Insufficient API quota for owner organization\"}\n",
      "WARNING: Unstructured could not process file data/c3fb282d-0de4-4d5f-87cf-83dbce5e6c12.pdf. Error: API error occurred: Status 402\n",
      "{\"detail\":\"Insufficient API quota for owner organization\"}\n",
      "Converting files to Haystack Documents: 3it [00:04,  1.50s/it]\n",
      "INFO: Running component sparse_doc_embedder\n",
      "Calculating sparse embeddings: 0it [00:00, ?it/s]\n",
      "INFO: Running component dense_doc_embedder\n",
      "Calculating embeddings: 0it [00:00, ?it/s]\n",
      "INFO: Running component writer\n",
      "INFO: HTTP Request: GET http://localhost:6333/collections/haystack_index/exists \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: DELETE http://localhost:6333/collections/haystack_index \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: PUT http://localhost:6333/collections/haystack_index \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET http://localhost:6333/collections/haystack_index/exists \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET http://localhost:6333/collections/haystack_index \"HTTP/1.1 200 OK\"\n",
      "WARNING: Calling QdrantDocumentStore.write_documents() with empty list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack_integrations.components.converters.unstructured import UnstructuredFileConverter\n",
    "from haystack_integrations.components.embedders.fastembed import FastembedSparseDocumentEmbedder, FastembedDocumentEmbedder\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantHybridRetriever\n",
    "from haystack_integrations.components.embedders.fastembed import FastembedTextEmbedder, FastembedSparseTextEmbedder\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack import Document, Pipeline\n",
    "\n",
    "qdrant_db_sparse = QdrantDocumentStore(\n",
    "    url=\"http://localhost:6333\",  # Adjust this if your Qdrant is hosted elsewhere\n",
    "    index=\"haystack_index\",      # Use the name of your existing Qdrant index\n",
    "    recreate_index=True,         # Ensure we don't overwrite the existing database\n",
    "    embedding_dim=384,\n",
    "    return_embedding=True,       # Return embeddings from Qdrant\n",
    "    use_sparse_embeddings=True,\n",
    "    sparse_idf=True\n",
    ")\n",
    "\n",
    "doc_embedder = FastembedDocumentEmbedder(model=\"BAAI/bge-small-en-v1.5\")\n",
    "doc_embedder.warm_up()\n",
    "sparse_doc_embedder = FastembedSparseDocumentEmbedder(model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n",
    "sparse_doc_embedder.warm_up()\n",
    "\n",
    "\n",
    "hybrid_indexing = Pipeline()\n",
    "hybrid_indexing.add_component(\"converter\", UnstructuredFileConverter(\n",
    "    api_url=\"https://api.unstructuredapp.io/general/v0/general\",\n",
    "    document_creation_mode=\"one-doc-per-element\"\n",
    "))\n",
    "hybrid_indexing.add_component(\"sparse_doc_embedder\", FastembedSparseDocumentEmbedder(model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\"))\n",
    "hybrid_indexing.add_component(\"dense_doc_embedder\", FastembedDocumentEmbedder(model=\"BAAI/bge-small-en-v1.5\"))\n",
    "hybrid_indexing.add_component(\"writer\", DocumentWriter(document_store=qdrant_db_sparse, policy=DuplicatePolicy.OVERWRITE))\n",
    "\n",
    "hybrid_indexing.connect(\"converter\", \"sparse_doc_embedder\")\n",
    "hybrid_indexing.connect(\"sparse_doc_embedder\", \"dense_doc_embedder\")\n",
    "hybrid_indexing.connect(\"dense_doc_embedder\", \"writer\")\n",
    "\n",
    "hybrid_indexing.run({\"paths\": filepaths})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Warming up component sparse_text_embedder...\n",
      "INFO: Warming up component dense_text_embedder...\n",
      "INFO: Running component sparse_text_embedder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was Fernando Eid born?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating sparse embeddings: 100%|██████████| 1/1 [00:00<00:00, 154.33it/s]\n",
      "INFO: Running component dense_text_embedder\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00, 16.62it/s]\n",
      "INFO: Running component retriever\n",
      "INFO: HTTP Request: GET http://localhost:6333/collections/haystack_index/exists \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: GET http://localhost:6333/collections/haystack_index \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://localhost:6333/collections/haystack_index/points/query \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO: Warming up component sparse_text_embedder...\n",
      "INFO: Warming up component dense_text_embedder...\n",
      "INFO: Running component sparse_text_embedder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the altitude range of the Zbrašov aragonite caves?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating sparse embeddings: 100%|██████████| 1/1 [00:00<00:00, 177.96it/s]\n",
      "INFO: Running component dense_text_embedder\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s]\n",
      "INFO: Running component retriever\n",
      "INFO: HTTP Request: POST http://localhost:6333/collections/haystack_index/points/query \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO: Warming up component sparse_text_embedder...\n",
      "INFO: Warming up component dense_text_embedder...\n",
      "INFO: Running component sparse_text_embedder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was the new station opened?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating sparse embeddings: 100%|██████████| 1/1 [00:00<00:00, 276.03it/s]\n",
      "INFO: Running component dense_text_embedder\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00, 13.97it/s]\n",
      "INFO: Running component retriever\n",
      "INFO: HTTP Request: POST http://localhost:6333/collections/haystack_index/points/query \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.components.converters.unstructured import UnstructuredFileConverter\n",
    "from haystack_integrations.components.embedders.fastembed import FastembedSparseDocumentEmbedder, FastembedDocumentEmbedder\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantHybridRetriever\n",
    "from haystack_integrations.components.embedders.fastembed import FastembedTextEmbedder, FastembedSparseTextEmbedder\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack import Document, Pipeline\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "import csv\n",
    "\n",
    "qdrant_db_sparse = QdrantDocumentStore(\n",
    "    url=\"http://localhost:6333\",  # Adjust this if your Qdrant is hosted elsewhere\n",
    "    index=\"haystack_index\",      # Use the name of your existing Qdrant index\n",
    "    recreate_index=False,         # Ensure we don't overwrite the existing database\n",
    "    return_embedding=True,       # Return embeddings from Qdrant\n",
    "    use_sparse_embeddings=True,\n",
    "    sparse_idf=True,\n",
    "    embedding_dim=384\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Answer the question based on the provided information. Answer concisely and informatively. If you don't know the answer, say so.\"\n",
    "generation_kwargs = {\n",
    "    \"seed\": 42,\n",
    "    # \"temperature\": 0.8,\n",
    "    # \"repeat_penalty\": 1.1,\n",
    "    # \"num_predict\": 128, # max number of tokens to generate\n",
    "    # \"top_k\": 50, # top-k sampling\n",
    "    # \"top_p\": 0.9, # top-p sampling\n",
    "    # \"min_p\": 0.0 # filter out token with probability less than this\n",
    "}\n",
    "generator = OllamaGenerator(model=\"llama3.2:1b\",\n",
    "                            url = \"http://localhost:11434\",\n",
    "                            system_prompt=system_prompt,\n",
    "                            generation_kwargs=generation_kwargs)\n",
    "\n",
    "doc_embedder = FastembedDocumentEmbedder(model=\"BAAI/bge-small-en-v1.5\")\n",
    "doc_embedder.warm_up()\n",
    "sparse_doc_embedder = FastembedSparseDocumentEmbedder(model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n",
    "sparse_doc_embedder.warm_up()\n",
    "\n",
    "hybrid_query = Pipeline()\n",
    "hybrid_query.add_component(\"sparse_text_embedder\", FastembedSparseTextEmbedder(model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\"))\n",
    "hybrid_query.add_component(\"dense_text_embedder\", FastembedTextEmbedder(model=\"BAAI/bge-small-en-v1.5\", prefix=\"Represent this sentence for searching relevant passages: \"))\n",
    "hybrid_query.add_component(\"retriever\", QdrantHybridRetriever(document_store=qdrant_db_sparse, top_k=5))\n",
    "\n",
    "hybrid_query.connect(\"sparse_text_embedder.sparse_embedding\", \"retriever.query_sparse_embedding\")\n",
    "hybrid_query.connect(\"dense_text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{sources_text}\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "with open(\"final.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"question\",\"ground_truth\",\"gen_answer\"])\n",
    "    for i,question  in enumerate(questions):\n",
    "        print(question)\n",
    "        results = hybrid_query.run(\n",
    "            {\"dense_text_embedder\": {\"text\": question},\n",
    "            \"sparse_text_embedder\": {\"text\": question}}\n",
    "        )\n",
    "        sources = [result.content for result in results[\"retriever\"][\"documents\"]]\n",
    "        prompt = prompt_template.format(sources_text=\"\\n\\n\".join(sources), question=question)\n",
    "\n",
    "        gen_answer = generator.run(prompt, generation_kwargs=generation_kwargs)[\"replies\"]\n",
    "        writer.writerow([question, ground_truths[i], gen_answer[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
